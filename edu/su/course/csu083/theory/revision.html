<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>Revision - CSU583 - Shoolini U</title>
        <meta name="description" content="Embark on a comprehensive journey through the this revision of CSU583 at Shoolini University. ">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article>
                <h2 class="text-center">
                    Revision
                </h2>
                <div class="container mt-4 w-100 w-xl-75">
                    <div class="accordion" id="toc">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="h1">
                                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#c1" aria-controls="c1" aria-expanded="false">
                                    <i class="fas fa-book"></i> <strong>&nbsp;Table of Contents</strong>
                                </button>
                            </h2>
                            <div id="c1" class="accordion-collapse collapse" aria-labelledby="h1" data-bs-parent="#toc">
                                <div class="accordion-body">
                                    <ol class="list-unstyled p-0 m-0">
                                        <li class="p-1"><a href="#prerequisites"><i class="fas fa-chevron-circle-right"></i> Prerequisites</a></li>
                                        <li class="p-1"><a href="#intro"><i class="fas fa-chevron-circle-right"></i> ACBT</a></li>
                                        <li class="p-1"><a href="#binary-tree-basics"><i class="fas fa-chevron-circle-right"></i> Binary Tree Basics</a></li>
                                        <li class="p-1"><a href="#tree-traversals"><i class="fas fa-chevron-circle-right"></i> Tree Traversals</a></li>
                                        <li class="p-1"><a href="#data-structure-implementation"><i class="fas fa-chevron-circle-right"></i> Data Structure Implementation</a></li>
                                        <li class="p-1"><a href="#time-and-space-complexity-analysis"><i class="fas fa-chevron-circle-right"></i> Mathematical Annotation and Time and Space Complexity Analysis</a></li>
                                        <li class="p-1"><a href="#balancing-and-rebalancing-techniques"><i class="fas fa-chevron-circle-right"></i> Balancing and Rebalancing Techniques</a></li>
                                        <li class="p-1"><a href="#binary-heap-and-priority-queue"><i class="fas fa-chevron-circle-right"></i> Binary Heap and Priority Queue</a></li>
                                        <li class="p-1"><a href="#dynamic-arrays-and-amortized-analysis"><i class="fas fa-chevron-circle-right"></i> Dynamic Arrays and Amortized Analysis</a></li>
                                        <li class="p-1"><a href="#tree-concepts"><i class="fas fa-chevron-circle-right"></i> Tree Concepts</a></li>
                                        <li class="p-1"><a href="#algorithm-optimization"><i class="fas fa-chevron-circle-right"></i> Algorithm Optimization</a></li>
                                        <li class="p-1"><a href="#memory-management"><i class="fas fa-chevron-circle-right"></i> Memory Management</a></li>
                                        <li class="p-1"><a href="#graph-theory-and-network-flows"><i class="fas fa-chevron-circle-right"></i> Graph Theory and Network Flows</a></li>
                                        <li class="p-1"><a href="#data-structures"><i class="fas fa-chevron-circle-right"></i> Data Structures</a></li>
                                        <li class="p-1"><a href="#concurrent-and-parallel-computing"><i class="fas fa-chevron-circle-right"></i> Concurrent and Parallel Computing</a></li>
                                        <li class="p-1"><a href="#data-structures"><i class="fas fa-chevron-circle-right"></i> Data Structures</a></li>
                                        <li class="p-1"><a href="#computational-complexity"><i class="fas fa-chevron-circle-right"></i> Computational Complexity</a></li>
                                        <li class="p-1"><a href="#practical-applications-and-case-studies"><i class="fas fa-chevron-circle-right"></i> Practical Applications and Case Studies</a></li>
                                        <li class="p-1"><a href="#conclusion"><i class="fas fa-chevron-circle-right"></i> Conclusion</a></li>
                                    </ol>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </article>

            <article id="overview-merging-algorithm">
                <h3 class="text-center display-6">Merging Algorithm</h3>
                <hr>
                <p>The Merging Algorithm is a fundamental part of Merge Sort, used to combine two sorted arrays or subarrays into a single sorted array. The basic idea is straightforward: compare the smallest (or largest, depending on the order of sorting) elements of the two arrays, and place the smaller (or larger) element into the new array, then move the index of the array from which the element was taken. This process continues until all elements from both arrays are merged into the new array.</p>
                <article>
                    <h4>Procedure of Merging Algorithm</h4>
                    <p>The merging algorithm involves the following steps:</p>
                    <ul>
                        <li><strong>Initial Setup:</strong> Two pointers are set up, each pointing to the start of the two arrays to be merged.</li>
                        <li><strong>Comparison and Merging:</strong> At each step, the elements pointed by the pointers are compared. The smaller (or larger, for descending order) of the two is placed into the new array, and the pointer of that array is advanced by one.</li>
                        <li><strong>Handling Remaining Elements:</strong> Once one of the arrays is exhausted, the remaining elements of the other array are copied into the new array.</li>
                        <li><strong>Output:</strong> The result is a fully merged and sorted array.</li>
                    </ul>
                </article>
                <article>
                    <h4>Recursion Relation of Merging Algorithm</h4>
                    <p>The recursion relation of the Merging Algorithm, particularly in the context of Merge Sort, is an essential aspect of understanding its efficiency and operation. The process of merging two sorted subarrays forms the basis of the Merge Sort algorithm, and its recursion relation can be described as follows:</p>
                    <ul>
                        <li><strong>Divide:</strong> The array is divided into two halves. If the size of the array is \( n \), each half will roughly be of size \( \frac{n}{2} \).</li>
                        <li><strong>Conquer:</strong> Recursively, the algorithm sorts both halves of the array. The recursion relation for this part is \( 2T(\frac{n}{2}) \) where \( T(n) \) is the time complexity of sorting an array of size \( n \).</li>
                        <li><strong>Merge:</strong> The two sorted halves are then merged together, which takes \( O(n) \) time, leading to the overall recursion relation of Merge Sort being \( T(n) = 2T(\frac{n}{2}) + O(n) \).</li>
                    </ul>
                    <p>This recursion relation explains how the algorithm divides the problem into smaller subproblems, solves them independently, and combines their results in a linear time complexity to achieve the overall sorting.</p>
                </article>
                <article>
                    <h4>Time Complexity of Merging Algorithm</h4>
                    <p>The time complexity of the merging algorithm is linear with respect to the total number of elements in the arrays being merged:</p>
                    <ul>
                        <li><strong>Time Complexity:</strong> \( O(n) \), where \( n \) is the total number of elements in the two arrays. This is because each element is looked at once.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Merging Algorithm</h4>
                    <p>The space complexity of the merging algorithm also depends on the total number of elements:</p>
                    <ul>
                        <li><strong>Space Complexity:</strong> \( O(n) \), as it requires additional space to store the merged array. This is separate from the space used by the input arrays.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-merge-sort">
                <h3 class="text-center display-6">Merge Sort</h3>
                <hr>
                <p>Merge Sort is an efficient, stable, comparison-based, divide-and-conquer sorting algorithm. Most implementations produce a stable sort, meaning that the implementation preserves the input order of equal elements in the sorted output. The algorithm divides the unsorted list into n sublists, each containing one element (a list of one element is considered sorted). Then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining — this will be the sorted list.</p>
                <article>
                    <h4>Recursion Relation of Merge Sort</h4>
                    <p>The recursion relation for Merge Sort is based on its divide-and-conquer strategy. It divides the array into two halves, sorts each half, and then merges them. The recurrence relation is \( T(n) = 2T(\frac{n}{2}) + \Theta(n) \), where \( T(n) \) is the time to sort an array of size \( n \). The division into halves continues until single-element arrays are reached, which are inherently sorted.</p>
                </article>
                <article>
                    <h4>Time Complexities of Merge Sort</h4>
                    <p>The time complexity of Merge Sort is consistent across various cases:</p>
                    <ul>
                        <li><strong>Worst-case:</strong> \( O(n \log n) \). This happens regardless of the initial arrangement of elements.</li>
                        <li><strong>Average-case:</strong> \( O(n \log n) \). It maintains this efficiency for all types of inputs.</li>
                        <li><strong>Best-case:</strong> \( O(n \log n) \). Even for already sorted arrays, it performs the same since it does not have a mechanism to detect sorted order.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Merge Sort</h4>
                    <p>Merge Sort's space complexity is higher compared to algorithms like Quick Sort:</p>
                    <ul>
                        <li><strong>Input Space:</strong> \( O(n) \). This is for the space used by the array to be sorted.</li>
                        <li><strong>Extra Space:</strong> \( O(n) \). Merge Sort requires additional space to store the temporary arrays during merging.</li>
                        <li><strong>Total Space Complexity:</strong> \( O(n) + O(n) = O(n) \). The total space requirement is linear with respect to the size of the input array.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-partition-algorithm">
                <h3 class="text-center display-6">Partition Algorithm</h3>
                <hr>
                <p>The Partition Algorithm is an integral part of the Quick Sort algorithm. It rearranges the elements of an array so that all elements less than a chosen pivot element are moved to its left, and all greater elements are moved to its right. The choice of the pivot and the method of partition can significantly affect the performance of Quick Sort. The partitioning process ensures that, after each pass, the pivot is in its final sorted position.</p>
                <article>
                    <h4>Procedure of Partition Algorithm</h4>
                    <p>The standard procedure for the Partition Algorithm involves the following steps:</p>
                    <ul>
                        <li><strong>Choosing a Pivot:</strong> Select an element as the pivot. Common strategies include choosing the first element, the last element, the median, or a random element.</li>
                        <li><strong>Rearranging Elements:</strong> Elements are rearranged so that those less than the pivot are on the left, and those greater are on the right. This is usually done using two pointers that scan the array from opposite ends.</li>
                        <li><strong>Finalizing Pivot Position:</strong> The pivot is then swapped with the element at the partition index, ensuring that it's placed in its correct sorted position.</li>
                    </ul>
                </article>
                <article>
                    <h4>Recursion Relation of Partition Algorithm</h4>
                    <p>The recursion relation for the Partition Algorithm, particularly in the context of Quick Sort, is key to understanding its performance. The Partition Algorithm is used to divide the array into subarrays, and its efficiency impacts the overall efficiency of Quick Sort. The recursion relation can be described as follows:</p>
                    <ul>
                        <li><strong>Partition:</strong> The Partition Algorithm selects a pivot and partitions the array into two subarrays: elements less than the pivot and elements greater than the pivot. The size of these subarrays depends on the pivot's position.</li>
                        <li><strong>Recursive Quick Sort:</strong> After partitioning, Quick Sort is recursively applied to each subarray. If the pivot divides the array into two subarrays of size \( m \) and \( n-m-1 \) (excluding the pivot), the recursion relation is \( T(n) = T(m) + T(n-m-1) + \Theta(n) \).</li>
                        <li><strong>Worst-Case Scenario:</strong> In the worst case, where the pivot is always the smallest or largest element, the relation becomes \( T(n) = T(n-1) + \Theta(n) \), leading to a time complexity of \( O(n^2) \).</li>
                        <li><strong>Average-Case Scenario:</strong> With a good pivot selection strategy, the array is more likely to be divided into two nearly equal parts, leading to the average-case relation \( T(n) = 2T(\frac{n}{2}) + \Theta(n) \), akin to Merge Sort, with an average time complexity of \( O(n \log n) \).</li>
                    </ul>
                    <p>This recursion relation demonstrates how the efficiency of the Partition Algorithm, especially the choice of the pivot, significantly impacts the overall efficiency of Quick Sort.</p>
                </article>
                <article>
                    <h4>Time Complexity of Partition Algorithm</h4>
                    <p>The time complexity of the Partition Algorithm is linear:</p>
                    <ul>
                        <li><strong>Time Complexity:</strong> \( O(n) \), where \( n \) is the number of elements in the array segment being partitioned. Each element is compared with the pivot at most once.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Partition Algorithm</h4>
                    <p>The space complexity of the Partition Algorithm is constant:</p>
                    <ul>
                        <li><strong>Space Complexity:</strong> \( O(1) \). The partitioning is done in-place, requiring no additional space beyond the input array.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-quick-sort">
                <h3 class="text-center display-6">Quick Sort</h3>
                <hr>
                <p>Quick Sort is a highly efficient sorting algorithm that uses a divide-and-conquer approach. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.</p>
                <article>
                    <h4>Recursion Relation of Quick Sort</h4>
                    <p>The recursion relation for Quick Sort depends on the pivot selection. The worst-case scenario is when the pivot is the smallest or largest element, leading to an unbalanced partition. In this case, the recurrence relation is \( T(n) = T(n-1) + \Theta(n) \), where \( T(n) \) is the time to sort an array of size \( n \). However, with a good pivot selection strategy, such as choosing the median, the average-case recurrence relation becomes \( T(n) = 2T(\frac{n}{2}) + \Theta(n) \).</p>
                </article>
                <article>
                    <h4>Time Complexities of Quick Sort</h4>
                    <p>The time complexity of Quick Sort varies:</p>
                    <ul>
                        <li><strong>Worst-case:</strong> \( O(n^2) \). This occurs when the smallest or largest element is always picked as the pivot.</li>
                        <li><strong>Average-case:</strong> \( O(n \log n) \). This is achieved with a good pivot selection, like choosing the median or using randomized pivoting.</li>
                        <li><strong>Best-case:</strong> \( O(n \log n) \). Occurs when the pivot divides the array into two equal halves.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Quick Sort</h4>
                    <p>Quick Sort's space complexity includes both the space used for the input array and the extra space required for the recursive calls:</p>
                    <ul>
                        <li><strong>Input Space:</strong> \( O(n) \). This is the space used by the array to be sorted.</li>
                        <li><strong>Extra Space:</strong> Depends on the implementation. In the worst case, it requires \( O(n) \) extra space for the recursive call stack, but it can be optimized to \( O(\log n) \) with tail call optimization or choosing a good pivot.</li>
                        <li><strong>Total Space Complexity:</strong> In the worst case, it's \( O(n) + O(n) = O(n) \) but can be optimized to \( O(n) + O(\log n) = O(n) \) in the best case.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-selection-procedure">
                <h3 class="text-center display-6">Selection Procedure</h3>
                <hr>
                <p>The Selection Procedure is a fundamental concept in various sorting and searching algorithms. It refers to the process of finding the k-th smallest (or largest) element in an array or list. This procedure is crucial in algorithms like Quick Select, which is used in Quick Sort for choosing the pivot. The efficiency of the selection process can significantly impact the overall performance of the algorithm it's part of.</p>
                <article>
                    <h4>Procedure of Selection</h4>
                    <p>The typical procedure for selecting the k-th smallest element includes:</p>
                    <ul>
                        <li><strong>Choosing a Pivot:</strong> Select an element as the pivot, similar to the partition step in Quick Sort.</li>
                        <li><strong>Partitioning:</strong> Rearrange the array so that all elements less than the pivot come before it, while all greater elements come after it. After partitioning, the pivot is in its final position.</li>
                        <li><strong>Target Position Check:</strong> If the pivot's position matches the k-th position, it's the desired element. If it's less, repeat the process for the right part of the array; if more, for the left part.</li>
                    </ul>
                </article>
                <article>
                    <h4>Recursion Relation of Selection Procedure</h4>
                    <p>The recursion relation for the Selection Procedure, particularly in the context of algorithms like Quick Select, is crucial for understanding its efficiency in selecting the k-th smallest or largest element. The recursion relation for the Selection Procedure can be described as follows:</p>
                    <ul>
                        <li><strong>Partitioning:</strong> Similar to the Quick Sort's partition algorithm, a pivot is selected, and the array is partitioned around it. If the pivot's position after partitioning is \( j \), it splits the array into two parts: elements less than the pivot and elements greater than the pivot.</li>
                        <li><strong>Recursive Selection:</strong> If \( k = j \), the pivot is the k-th smallest element. If \( k < j \), the process is recursively applied to the left subarray; if \( k> j \), it is applied to the right subarray. This leads to the recursion relation \( T(n) = T(n/2) + O(n) \) on average, assuming the pivot splits the array into two equal parts.</li>
                        <li><strong>Worst-Case Scenario:</strong> In the worst case, if the smallest or largest element is always chosen as the pivot, the recursion relation becomes \( T(n) = T(n-1) + O(n) \), similar to the worst case of Quick Sort, leading to a time complexity of \( O(n^2) \).</li>
                        <li><strong>Average-Case Scenario:</strong> With a good pivot, the expected time complexity is \( O(n) \), making it an efficient algorithm for selection problems.</li>
                    </ul>
                    <p>This recursion relation highlights how the selection procedure efficiently reduces the problem size at each step, particularly in the average case, leading to a significant reduction in the number of comparisons and operations required to find the k-th smallest or largest element.</p>
                </article>
                <article>
                    <h4>Time Complexity of Selection Procedure</h4>
                    <p>The time complexity of the Selection Procedure varies:</p>
                    <ul>
                        <li><strong>Worst-case:</strong> \( O(n^2) \), particularly if the pivot selection is poor leading to unbalanced partitions.</li>
                        <li><strong>Average-case:</strong> \( O(n) \), achieved with good pivot selection, as the procedure typically discards half of the elements at each step.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Selection Procedure</h4>
                    <p>The space complexity of the Selection Procedure is:</p>
                    <ul>
                        <li><strong>Space Complexity:</strong> \( O(1) \) for the in-place variant. The selection is done within the array itself, requiring no additional significant space.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-heapify-algorithm">
                <h3 class="text-center display-6">Heapify Algorithm</h3>
                <hr>
                <p>The Heapify Algorithm is crucial in building and maintaining a heap data structure, which is an essential part of Heap Sort and priority queue implementations. A heap can be a max-heap or min-heap, where the value of each node is greater (max-heap) or smaller (min-heap) than its children. The heapify process ensures that the tree meets the heap property. It's typically applied to a node and ensures that the subtree rooted at this node satisfies the heap property, assuming its children already do.</p>
                <article>
                    <h4>Procedure of Heapify Algorithm</h4>
                    <p>The Heapify Algorithm involves:</p>
                    <ul>
                        <li><strong>Identifying the Violation:</strong> Check if the current node violates the heap property with respect to its children.</li>
                        <li><strong>Swapping Elements:</strong> If the property is violated, swap the node with its largest child (max-heap) or smallest child (min-heap).</li>
                        <li><strong>Recursive Heapify:</strong> Apply heapify recursively to the subtree where the swap was made, ensuring the heap property is maintained throughout.</li>
                    </ul>
                </article>
                <article>
                    <h4>Recursion Relation of Heapify Algorithm</h4>
                    <p>The recursion relation of the Heapify Algorithm is central to its function in maintaining the heap property in a heap data structure. The Heapify Algorithm's recursion relation can be understood as follows:</p>
                    <ul>
                        <li><strong>Heap Property Violation:</strong> When a node violates the heap property (i.e., it is smaller than its children in a max-heap or larger in a min-heap), a swap is required to restore the property.</li>
                        <li><strong>Recursive Downward Traversal:</strong> After swapping the violating node with its largest (or smallest) child, the heapify process needs to be applied recursively to the subtree rooted at the child node where the swap occurred. This is because the swap might have introduced a violation of the heap property at the next level down.</li>
                        <li><strong>Recursive Relation:</strong> The recursion continues down the height of the heap. If the height of the heap is \( h \), the maximum number of recursive calls (in the worst case) is \( h \), leading to the recursion relation of \( T(h) = T(h-1) + O(1) \), where each level down the heap takes constant time, assuming a binary heap.</li>
                        <li><strong>Height of Heap:</strong> For a binary heap, the height \( h \) is \( \log n \), where \( n \) is the number of nodes. Thus, the overall time complexity for heapify becomes \( O(\log n) \).</li>
                    </ul>
                    <p>This recursion relation underpins the logarithmic time complexity of the Heapify Algorithm and explains how it efficiently maintains the heap property throughout a heap.</p>
                </article>
                <article>
                    <h4>Time Complexity of Heapify Algorithm</h4>
                    <p>The time complexity of the Heapify Algorithm is:</p>
                    <ul>
                        <li><strong>Time Complexity:</strong> \( O(\log n) \), where \( n \) is the number of nodes in the heap. This logarithmic time complexity comes from the need to potentially traverse a path from the root to a leaf.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Heapify Algorithm</h4>
                    <p>The space complexity of the Heapify Algorithm is:</p>
                    <ul>
                        <li><strong>Space Complexity:</strong> \( O(1) \) for the in-place variant, as it requires no additional space beyond the input array that represents the heap.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-max-heap-min-heap">
                <h3 class="text-center display-6">Max Heap and Min Heap</h3>
                <hr>
                <p>Max Heap and Min Heap are specialized tree-based data structures that satisfy the heap property. In a Max Heap, for any given node, its value is always greater than or equal to the values of its children. Conversely, in a Min Heap, the value of any node is always less than or equal to its children. These properties make heaps useful for implementing priority queues, where the order of objects is determined by their priority.</p>
                <article>
                    <h4>Characteristics of Max Heap</h4>
                    <p>A Max Heap has the following characteristics:</p>
                    <ul>
                        <li><strong>Parent Node's Value:</strong> Each parent node has a value greater than or equal to those of its children.</li>
                        <li><strong>Structure:</strong> It is a complete binary tree, meaning all levels of the tree are fully filled except possibly the last level, which is filled from left to right.</li>
                        <li><strong>Applications:</strong> Used in algorithms like Heap Sort, and for implementing efficient priority queues.</li>
                    </ul>
                </article>
                <article>
                    <h4>Characteristics of Min Heap</h4>
                    <p>A Min Heap is characterized by:</p>
                    <ul>
                        <li><strong>Parent Node's Value:</strong> Each parent node has a value less than or equal to those of its children.</li>
                        <li><strong>Structure:</strong> Like the Max Heap, it is also a complete binary tree.</li>
                        <li><strong>Applications:</strong> Commonly used in algorithms for finding the minimum element, Dijkstra's algorithm, and Prim's algorithm for the minimum spanning tree.</li>
                    </ul>
                </article>
                <article>
                    <h4>Representation in Memory</h4>
                    <p>Both Max Heap and Min Heap are often represented in memory as arrays, where if a parent node is at index \( i \), its children are at indices \( 2i+1 \) (left child) and \( 2i+2 \) (right child), and its parent (if any) is at \( \lfloor (i-1)/2 \rfloor \). This representation is efficient and simplifies the algorithms for insertion and deletion in a heap.</p>
                </article>
                <article>
                    <h4>Recursion Relation of Max Heap and Min Heap</h4>
                    <p>While Max Heaps and Min Heaps themselves are data structures and not algorithms, they are fundamental to the operation of several heap-based algorithms. The recursion relation in the context of Max Heaps and Min Heaps typically applies to the procedures used for their construction and maintenance, such as the Heapify Algorithm, and the procedures for inserting and deleting elements. These operations are recursive in nature and can be understood as follows:</p>
                    <ul>
                        <li><strong>Heapify Process:</strong> As discussed previously, the Heapify operation, essential for maintaining the heap property after insertion or deletion, follows a \( O(\log n) \) time complexity due to its recursive nature of traversing downwards from the root to the leaf level.</li>
                        <li><strong>Insertion:</strong> When a new element is added, it is initially placed at the bottom of the heap (the next available spot in the complete binary tree structure). The element is then recursively compared and possibly swapped with its parent nodes until the heap property is restored. This upward traversal has a recursion relation similar to Heapify, bounded by the height of the heap.</li>
                        <li><strong>Deletion:</strong> Deletion, particularly of the root element (which is the maximum in a max heap or minimum in a min heap), involves replacing the root with the last element and then applying the Heapify operation to restore the heap property, following a \( O(\log n) \) recursion relation.</li>
                    </ul>
                    <p>In summary, the recursive nature of these heap operations ensures efficient maintenance of the heap property, crucial for the performance of heap-based algorithms and data structures.</p>
                </article>
            </article>

            <article id="overview-heap-sort">
                <h3 class="text-center display-6">Heap Sort</h3>
                <hr>
                <p>Heap Sort is a comparison-based sorting technique based on the Binary Heap data structure. It's similar to Selection Sort where we first find the maximum (or minimum) element and place the maximum at the end (or start) of the array. We then repeat the process for the remaining elements. Heap Sort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest (or smallest) element and moving that to the sorted region.</p>
                <article>
                    <h4>Procedure of Heap Sort</h4>
                    <p>The Heap Sort algorithm involves two main phases:</p>
                    <ul>
                        <li><strong>Building a Heap:</strong> Transform the array into a max heap (for ascending sort) or min heap (for descending sort).</li>
                        <li><strong>Sorting:</strong> Repeatedly remove the largest element from the heap and place it at the end of the array, then re-heapify the remainder of the heap. Repeat until all elements are sorted.</li>
                    </ul>
                </article>
                <article>
                    <h4>Recursion Relation of Heap Sort</h4>
                    <p>The Heap Sort algorithm, though not typically described in terms of recursion like Merge Sort, does have an underlying structure that can be understood in terms of repetitive operations. Its process can be broken down as follows:</p>
                    <ul>
                        <li><strong>Building the Heap:</strong> The initial step of building a heap from an array of \( n \) elements can be seen as applying the heapify operation to each element, starting from the last non-leaf node to the root. This step takes \( O(n) \) time, as heapify is a \( O(\log n) \) operation and it's applied \( n/2 \) times.</li>
                        <li><strong>Extracting Elements:</strong> After building the heap, the algorithm repeatedly removes the root (maximum/minimum element) and re-heapifies the remaining elements. This step is repeated \( n \) times, and each heapify operation takes \( O(\log n) \) time, leading to a total time of \( O(n \log n) \) for this phase.</li>
                    </ul>
                    <p>While not a recursion relation in the classic sense, this breakdown of Heap Sort into its constituent operations helps understand its time complexity and the iterative application of heapify.</p>
                </article>
                <article>
                    <h4>Time Complexity of Heap Sort</h4>
                    <p>The time complexity of Heap Sort is:</p>
                    <ul>
                        <li><strong>Worst-case:</strong> \( O(n \log n) \). The worst-case occurs when the array is already sorted in the opposite order.</li>
                        <li><strong>Average-case:</strong> \( O(n \log n) \). Similar to the worst-case, due to the properties of the heap.</li>
                        <li><strong>Best-case:</strong> \( O(n \log n) \). Even for a nearly sorted array, the time complexity remains the same because of the heap construction and maintenance steps.</li>
                    </ul>
                </article>
                <article>
                    <h4>Space Complexity of Heap Sort</h4>
                    <p>The space complexity of Heap Sort is minimal:</p>
                    <ul>
                        <li><strong>Space Complexity:</strong> \( O(1) \). Heap Sort is an in-place algorithm, so it doesn't require additional storage beyond what's needed for the original array.</li>
                    </ul>
                </article>
            </article>

            <article id="overview-almost-complete-binary-tree">
                <h3 class="text-center display-6">Almost Complete Binary Tree</h3>
                <hr>
                <p>An Almost Complete Binary Tree is a type of binary tree where all levels are completely filled except possibly the last level, and the last level has all its nodes as far left as possible. This structure differs slightly from a "complete binary tree" and is a key characteristic of heaps used in Heap Sort and priority queues. In an almost complete binary tree, the insertion of new nodes happens at the leftmost position available at the bottom level.</p>
                <article>
                    <h4>Characteristics of Almost Complete Binary Tree</h4>
                    <p>Key features include:</p>
                    <ul>
                        <li><strong>Level Filling:</strong> All levels, except possibly the last, are fully filled.</li>
                        <li><strong>Last Level Nodes:</strong> The nodes in the last level are as far left as possible, which means there can be no gaps in the sequence of nodes at the last level.</li>
                        <li><strong>Height Efficiency:</strong> Such trees have the smallest possible height for the number of nodes, which is efficient for storage and traversal.</li>
                    </ul>
                </article>
                <article>
                    <h4>Representation and Importance</h4>
                    <p>Almost Complete Binary Trees are typically represented in an array, where parent-child relationships can be computed using indices. This representation is compact and efficient, especially for algorithms like Heap Sort, where the tree needs to be frequently restructured.</p>
                </article>
                <article>
                    <h4>Formulas and Properties of Almost Complete Binary Tree</h4>
                    <p>Important formulas and properties include:</p>
                    <ul>
                        <li><strong>Height of the Tree:</strong> The height \( h \) of an almost complete binary tree with \( n \) nodes is given by \( h = \lfloor \log_2(n) \rfloor \). This represents the maximum number of edges in the path from the root to a leaf.</li>
                        <li><strong>Maximum Number of Nodes:</strong> The maximum number of nodes at a given level \( l \) (considering root level as 0) is \( 2^l \). Thus, the total number of nodes in an almost complete binary tree of height \( h \) is at most \( 2^{h+1} - 1 \).</li>
                        <li><strong>Minimum Number of Nodes:</strong> The minimum number of nodes at the last level to still qualify as an almost complete binary tree is 1. So, the minimum number of nodes in a tree of height \( h \) is \( 2^h \).</li>
                        <li><strong>Parent-Child Relationship:</strong> In an array representation, if a parent node is at index \( i \), then its left child is at \( 2i + 1 \) and its right child is at \( 2i + 2 \). Conversely, the parent of a node at index \( i \) is at \( \lfloor (i-1)/2 \rfloor \).</li>
                        <li><strong>Leaf Nodes:</strong> In an almost complete binary tree, the leaf nodes start from index \( \lfloor n/2 \rfloor \) to \( n - 1 \) in the array representation, where \( n \) is the total number of nodes.</li>
                    </ul>
                </article>
                <article>
                    <h4>Applications in Computer Science</h4>
                    <p>Almost complete binary trees are widely used in computer science, particularly in the implementation of binary heaps. This structure ensures efficient utilization of space and optimal performance for heap operations like insertion, deletion, and heapify, which are central to algorithms like Heap Sort and for the implementation of priority queues.</p>
                </article>
            </article>

            <article id="overview-complete-binary-tree">
                <h3 class="text-center display-6">Complete Binary Tree</h3>
                <hr>
                <hr>
                <p>A Complete Binary Tree is a type of binary tree where all levels are completely filled except possibly the last level, and the last level has all keys as left as possible. This structure ensures a balanced tree, making it efficient for operations like insertion, deletion, and search. It's widely used in heap-based algorithms and data structures like priority queues.</p>
            </article>
            <article>
                <h4>Characteristics of Complete Binary Tree</h4>
                <p>Key features of a Complete Binary Tree include:</p>
                <ul>
                    <li><strong>Level Filling:</strong> All levels are fully filled, except possibly for the last level.</li>
                    <li><strong>Last Level Filling:</strong> In the last level, nodes are filled from left to right without any gaps.</li>
                    <li><strong>Height Efficiency:</strong> The height of the tree is minimized for the number of nodes, which makes operations efficient.</li>
                </ul>
            </article>
            <article>
                <h4>Representation and Properties</h4>
                <p>A Complete Binary Tree is often represented as an array for efficient access and manipulation:</p>
                <ul>
                    <li><strong>Height of the Tree:</strong> The height \( h \) is \( \lfloor \log_2(n) \rfloor \), where \( n \) is the number of nodes.</li>
                    <li><strong>Node Indices:</strong> If a node is at index \( i \), its children are at \( 2i+1 \) (left) and \( 2i+2 \) (right), and its parent is at \( \lfloor (i-1)/2 \rfloor \).</li>
                    <li><strong>Number of Nodes:</strong> The number of nodes at level \( l \) is \( 2^l \), and the total number of nodes is between \( 2^h \) and \( 2^{h+1}-1 \).</li>
                </ul>
            </article>
            <article>
                <h4>Applications in Computer Science</h4>
                <p>Complete Binary Trees are essential in computer science, particularly in heap-based data structures and algorithms like Heap Sort and Priority Queues. Their balanced nature ensures efficient performance in these applications.</p>
            </article>

        </main>

        <script> copyright("all"); </script>

    </body>

</html>